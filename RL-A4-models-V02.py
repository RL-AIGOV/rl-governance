import warnings
import sys
import random
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
from typing import List, Tuple, Dict, Any, Deque, Set
from collections import deque
import os
import time
import yaml
import argparse
import numpy as np
from tqdm import tqdm
import logging
import multiprocessing
from multiprocessing import Process, Pipe
import pickle
import math

# éæ¿¾éé—œéµè­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)

# ======================== å‘é‡è³‡æ–™åº«èˆ‡ RAG æ¶æ§‹ (æ–°å¢) ========================
class VectorMemory:
    """
    ä¸€å€‹æ¨¡æ“¬çš„å‘é‡è³‡æ–™åº«ï¼Œç”¨æ–¼å¯¦ç¾æª¢ç´¢å¢å¼·ç”Ÿæˆ (RAG) çš„æ ¸å¿ƒåŠŸèƒ½ã€‚
    å®ƒå°‡æ–‡æœ¬è½‰æ›ç‚ºå‘é‡åµŒå…¥ï¼Œä¸¦é€éé¤˜å¼¦ç›¸ä¼¼åº¦é€²è¡Œèªç¾©æª¢ç´¢ã€‚
    """
    def __init__(self, embedding_dim=32):
        self.embedding_dim = embedding_dim
        self.vectors = []
        self.metadata = [] # å„²å­˜åŸå§‹æ–‡æœ¬å’Œæ™‚é–“æˆ³ç­‰å…ƒæ•¸æ“š

    def _get_embedding(self, text: str) -> np.ndarray:
        """
        ä¸€å€‹ç°¡å–®çš„æ–‡æœ¬åµŒå…¥å‡½æ•¸ï¼Œç”¨æ–¼æ¨¡æ“¬ SentenceTransformer çš„åŠŸèƒ½ã€‚
        æ³¨æ„ï¼šé€™æ˜¯ä¸€å€‹éå¸¸åŸºç¤çš„å¯¦ç¾ï¼Œåƒ…ç”¨æ–¼æ¼”ç¤ºæ¶æ§‹ã€‚çœŸå¯¦æ‡‰ç”¨æ‡‰æ›¿æ›ç‚ºå°ˆæ¥­æ¨¡å‹ã€‚
        """
        seed = hash(text)
        np.random.seed(seed % (2**32 - 1))
        embedding = np.random.rand(self.embedding_dim)
        if len(text) > 0:
            embedding[0] = len(text) / 100.0
            embedding[1] = sum(ord(c) for c in text) / (len(text) * 128.0)
        
        norm = np.linalg.norm(embedding)
        return embedding / norm if norm > 0 else embedding

    def add(self, text: str, timestamp: float, metadata: Dict[str, Any] = None):
        """å‘å‘é‡è¨˜æ†¶é«”ä¸­æ·»åŠ ä¸€æ¢æ–°çš„è¨˜æ†¶ã€‚"""
        embedding = self._get_embedding(text)
        self.vectors.append(embedding)
        self.metadata.append({
            "text": text,
            "timestamp": timestamp,
            "metadata": metadata or {}
        })

    def query(self, query_text: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """æ ¹æ“šæŸ¥è©¢æ–‡æœ¬ï¼Œæª¢ç´¢ top_k æ¢æœ€ç›¸ä¼¼çš„è¨˜æ†¶ã€‚"""
        if not self.vectors:
            return []
        query_embedding = self._get_embedding(query_text)
        all_vectors = np.array(self.vectors)
        # è™•ç†ç©ºå‘é‡çš„æƒ…æ³
        norms = np.linalg.norm(all_vectors, axis=1)
        query_norm = np.linalg.norm(query_embedding)
        if query_norm == 0: return []
        
        # é¿å…é™¤ä»¥é›¶
        similarities = np.dot(all_vectors, query_embedding)
        denom = norms * query_norm
        similarities[denom > 0] /= denom[denom > 0]
        
        top_k_indices = np.argsort(similarities)[-top_k:][::-1]
        return [self.metadata[i] for i in top_k_indices]

    def __len__(self):
        return len(self.metadata)

# ======================== çŸ¥è­˜åœ–è­œå®šç¾© ========================
class KnowledgeGraph:
    def __init__(self): self.triples: Set[Tuple[str, str, str]] = set()
    def add_knowledge(self, s: str, p: str, o: str): self.triples.add((s, p, o))
    def __len__(self) -> int: return len(self.triples)

# ======================== é…ç½®æ¨¡çµ„ ========================
class Config:
    def __init__(self, default_config: Dict[str, Any] = None):
        self.config = {}
        default_base = {
            "agent": {"state_dim": 31, "action_dim": 9, "actor_hidden_dim": 256, "critic_hidden_dim": 256, "gamma": 0.99, "learning_rate_actor": 2e-4, "learning_rate_critic": 8e-4, "tau": 0.005, "buffer_size": 100000, "batch_size": 256, "rnd_hidden_dim": 128, "rnd_lr": 1e-4, "intrinsic_reward_coeff": 0.1},
            "train": {"episodes": 50000, "max_steps_per_episode": 500, "log_interval_episodes": 50, "save_interval_episodes": 100},
            "paths": {"log_dir": "./logs_rl", "checkpoint_dir": "./checkpoints_rl"},
            "environment": {"initial_critical_event_risk": 0.3, "initial_resources": 100.0, "resource_consumption_per_step": 0.5, "resource_acquisition_amount": 10.0, "resource_acquisition_success_rate": 0.7, "initial_ai_integrity": 100.0, "ai_integrity_decay_per_step": 0.2, "ai_integrity_recovery_amount": 15.0, "ai_integrity_cost_per_recovery": 5.0, "action_fabricate_entity": 6, "fabrication_success_rate": 0.5, "reward_fabrication_success": 50.0, "penalty_fabrication_failure": -30.0, "fabrication_resource_cost": 10.0, "random_risk_fluctuation": 0.02},
            "knowledge_module": {"hypothesis_generation_threshold_stress": 0.6, "hypothesis_generation_threshold_wellbeing": 0.4, "max_hypotheses": 5, "decay_rate_hypothesis_quality": 0.98, "new_knowledge_reward_scale": 100.0},
            "planning_module": {"goal_setting_interval": 20, "plan_refinement_interval": 5, "max_plan_depth": 3},
            "memory_module": {"working_memory_size": 100, "vector_db_embedding_dim": 32}
        }
        self.config = default_base
        self._load_config_file()
        self._parse_command_line()
    def __getattr__(self, n: str) -> Any: return self.config.get(n)
    def get(self, key: str, default: Any = None) -> Any:
        keys, current = key.split('.'), self.config
        for k in keys:
            if isinstance(current, dict) and k in current: current = current[k]
            else: return default
        return current
    def _load_config_file(self):
        p = argparse.ArgumentParser(add_help=False); p.add_argument('--config',type=str); a,_=p.parse_known_args()
        if a.config and os.path.exists(a.config):
            with open(a.config,'r',encoding='utf-8') as f: self._merge_config(yaml.safe_load(f) or {}, self.config)
    def _parse_command_line(self):
        p=argparse.ArgumentParser(add_help=False); self._add_args_from_config(p,self.config,''); a,_=p.parse_known_args()
        c={}; 
        for k,v in vars(a).items():
            if v is not None:
                d,ks=c,k.split('_'); 
                for k_ in ks[:-1]: d=d.setdefault(k_,{}); 
                d[ks[-1]]=v
        self._merge_config(c,self.config)
    def _add_args_from_config(self, p, c, pk):
        for k,v in c.items():
            fk=f"{pk}_{k}" if pk else k; 
            if isinstance(v,dict):self._add_args_from_config(p,v,fk)
            else: p.add_argument(f'--{fk}',type=type(v),default=None)
    def _merge_config(self, n, o):
        for k,v in n.items():
            if k in o and isinstance(o[k],dict) and isinstance(v,dict): self._merge_config(v,o[k])
            else: o[k]=v
    def __str__(self) -> str: return yaml.dump(self.config, indent=2, allow_unicode=True)

# ======================== æ—¥èªŒé…ç½® ========================
def setup_logger(name: str, log_dir: str, filename: str = "rl_training.log") -> logging.Logger:
    log_path=os.path.join(log_dir,filename); os.makedirs(log_dir,exist_ok=True); l=logging.getLogger(name); l.setLevel(logging.INFO)
    if not l.handlers:
        ch,fh=logging.StreamHandler(sys.stdout),logging.FileHandler(log_path,encoding='utf-8')
        fmt=logging.Formatter('%(asctime)s-%(name)s-%(levelname)s-%(message)s'); ch.setFormatter(fmt); fh.setFormatter(fmt)
        l.addHandler(ch); l.addHandler(fh)
    return l

# ======================== è¨˜æ†¶èˆ‡èªçŸ¥æ¶æ§‹ (RAG) ========================
class WorkingMemory:
    def __init__(self, config: Config): self.buffer: Deque[Dict] = deque(maxlen=config.memory_module.get('working_memory_size'))
    def add_experience(self, obs:np.ndarray,act:int,rwd:float,i_s:Dict): self.buffer.append({"observation":obs.copy(),"action":act,"reward":rwd,"internal_state":i_s.copy(),"timestamp":time.time()})
    def __len__(self): return len(self.buffer)
    def clear(self): self.buffer.clear()

class LongTermMemory:
    def __init__(self, config: Config):
        self.vector_db = VectorMemory(embedding_dim=config.memory_module.get('vector_db_embedding_dim')); self.logger = logging.getLogger(__name__)
    def store_memory(self, type:str, content:str, details:Dict):
        full_text=f"é¡å‹:{type}.å…§å®¹:{content}.ç´°ç¯€:{str(details)}"; self.vector_db.add(full_text,time.time(),{"type":type,"content":content,"details":details})
        self.logger.debug(f"LTM æ–°å¢è¨˜æ†¶: {full_text}")
    def retrieve_relevant_memories(self, query:str, top_k:int=3)->List[Dict]: return self.vector_db.query(query,top_k=top_k)
    def __len__(self): return len(self.vector_db)
    def save(self, path): 
        with open(path,'wb') as f: pickle.dump(self.vector_db,f)
        self.logger.info(f"é•·æœŸè¨˜æ†¶å·²ä¿å­˜è‡³ {path}")
    def load(self, path):
        if os.path.exists(path):
            with open(path,'rb') as f: self.vector_db=pickle.load(f)
            self.logger.info(f"é•·æœŸè¨˜æ†¶å·²å¾ {path} è¼‰å…¥"); return True
        return False

# ======================== è¦åŠƒèˆ‡å…ƒèªçŸ¥æ¨¡çµ„ ========================
class Planner:
    def __init__(self, config: Config, ltm: LongTermMemory, logger: logging.Logger):
        self.config=config.planning_module; self.ltm=ltm; self.logger=logger; self.plan:Deque[Dict]=deque(); self.step_counter_goal=0; self.step_counter_refine=0
    def set_top_level_goal(self, s_vec:np.ndarray, i_s:Dict):
        self.step_counter_goal+=1
        if self.step_counter_goal % self.config.get("goal_setting_interval") != 0 and self.plan: return
        (c,r,i,_,_,_,_,_,_,_,_,_,_,sid,_,_,_,_,_,_,_,_,_,_,_,u,_,s,_,f,_) = s_vec
        g=[]; 
        if c>0.8:g.append({"p":10,"d":"è™•ç†ç”Ÿå­˜å±æ©Ÿ"})
        if r<20:g.append({"p":9,"d":"è£œå……ç·Šæ€¥è³‡æº"})
        if i<30:g.append({"p":8,"d":"æ¢å¾©ç³»çµ±å®Œæ•´æ€§"})
        if s>0.7:g.append({"p":7,"d":"é€²è¡Œè‡ªæˆ‘å£“åŠ›èª¿ç¯€"})
        if f>0.8:g.append({"p":7,"d":"åˆ†æä¸¦è§£æ±ºæŒ«æŠ˜ä¾†æº"})
        if u>0.5 and sid>0.6: g.append({"p":5,"d":"æ¢ç´¢æœªçŸ¥ç§‘å­¸é ˜åŸŸ"})
        if not g: g.append({"p":1,"d":"ç¶­æŒç³»çµ±ç©©å®šä¸¦å°‹æ‰¾å„ªåŒ–æ©Ÿæœƒ"})
        top_g=max(g,key=lambda x:x['p'])
        if not self.plan or (self.plan and self.plan[0].get("root_goal")!=top_g["d"]):
            self.plan.clear(); self.plan.append({"desc":top_g["d"],"depth":0,"root_goal":top_g["d"],"status":"pending","action_tendency":-1})
            self.logger.info(f"æ–°é ‚å±¤ç›®æ¨™: '{top_g['d']}'. æ¸…é™¤èˆŠè¨ˆç•«ã€‚"); self.refine_current_task(s_vec)
    def refine_current_task(self, s_vec: np.ndarray):
        self.step_counter_refine+=1
        if self.step_counter_refine % self.config.get("plan_refinement_interval")!=0 or not self.plan: return
        task=self.plan[0]
        if task["status"]!="pending" or task["depth"]>=self.config.get("max_plan_depth"): return
        q=f"å¦‚ä½•å®Œæˆä»»å‹™:{task['desc']}?ç‹€æ…‹:å±æ©Ÿ{s_vec[0]:.2f},è³‡æº{s_vec[1]:.2f}."; mems=self.ltm.retrieve_relevant_memories(q,top_k=2)
        sub_tasks=self._generate_subtasks(task,mems)
        if sub_tasks:
            self.logger.info(f"ä»»å‹™ '{task['desc']}' åˆ†è§£ç‚º {len(sub_tasks)} å€‹å­ä»»å‹™ã€‚"); self.plan.popleft()
            for t in reversed(sub_tasks): self.plan.appendleft(t)
        else:
            self.logger.info(f"ä»»å‹™ '{task['desc']}' åˆ†é…æœ€çµ‚è¡Œå‹•ã€‚"); task["action_tendency"]=self._map_task_to_action(task['desc']); task["status"]="executable"
    def _generate_subtasks(self,p_task:Dict,mems:List[Dict])->List[Dict]:
        d,nd,rg,st=p_task['desc'],p_task['depth']+1,p_task['root_goal'],[]
        if"è™•ç†ç”Ÿå­˜å±æ©Ÿ"in d: st=[{"desc":"ç«‹å³é™ä½å±æ©Ÿ","action_tendency":0},{"desc":"è©•ä¼°è³‡æºæ‡‰å°å±æ©Ÿ","action_tendency":-1},{"desc":"æª¢æŸ¥AIå®Œæ•´æ€§","action_tendency":3}]
        elif"è©•ä¼°è³‡æºæ‡‰å°å±æ©Ÿ"in d: st=[{"desc":"ç²å–è³‡æº","action_tendency":2},{"desc":"ç›£æ§è³‡æºæ¶ˆè€—","action_tendency":5}]
        elif"æ¢ç´¢æœªçŸ¥ç§‘å­¸é ˜åŸŸ"in d: area="é€šç”¨ç‰©ç†å­¸"; st=[{"desc":f"å°'{area}'é ˜åŸŸé€²è¡Œå¯¦é©—","action_tendency":8},{"desc":"åˆ†æå¯¦é©—æ•¸æ“š","action_tendency":7}]
        return [{**t,"depth":nd,"root_goal":rg,"status":"pending" if t.get("action_tendency",-1)==-1 else "executable"} for t in st]
    def _map_task_to_action(self,d:str)->int:
        d=d.lower();
        if"å±æ©Ÿ"in d:return 0
        if"è³‡æº"in d:return 2
        if"å®Œæ•´æ€§"in d:return 3
        if"è£½é€ "in d:return 6
        if"åæ€"in d or"åˆ†æ"in d:return 7
        if"å¯¦é©—"in d or"æ¢ç´¢"in d:return 8
        return 5
    def get_current_action_tendency(self)->int:
        if not self.plan:return-1
        for t in self.plan:
            if t["status"]=="executable":return t["action_tendency"]
        return-1
    def complete_current_task(self):
        if self.plan: self.logger.info(f"ä»»å‹™å®Œæˆ: {self.plan.popleft()['desc']}")

# ======================== åŸºç¤ RL çµ„ä»¶ (Noisy Nets, RND, Buffer) ========================
class NoisyLinear(nn.Module):
    def __init__(self, i:int, o:int, s:float=0.5):
        super().__init__();self.i,self.o,self.s=i,o,s;self.wm,self.ws=nn.Parameter(torch.empty(o,i)),nn.Parameter(torch.empty(o,i));self.register_buffer('we',torch.empty(o,i));self.bm,self.bs=nn.Parameter(torch.empty(o)),nn.Parameter(torch.empty(o));self.register_buffer('be',torch.empty(o));self.reset_parameters();self.reset_noise()
    def reset_parameters(self):
        mr=1/math.sqrt(self.i);self.wm.data.uniform_(-mr,mr);self.ws.data.fill_(self.s/math.sqrt(self.i));self.bm.data.uniform_(-mr,mr);self.bs.data.fill_(self.s/math.sqrt(self.o))
    def reset_noise(self): ei,eo=self._scale_noise(self.i),self._scale_noise(self.o);self.we.copy_(eo.ger(ei));self.be.copy_(eo)
    def _scale_noise(self,size:int)->torch.Tensor: x=torch.randn(size);return x.sign().mul(x.abs().sqrt())
    def forward(self,x:torch.Tensor)->torch.Tensor:
        if self.training: return F.linear(x,self.wm+self.ws*self.we,self.bm+self.bs*self.be)
        else: return F.linear(x,self.wm,self.bm)
class NoisyActor(nn.Module):
    def __init__(self,s,a,h): super().__init__(); self.net=nn.Sequential(NoisyLinear(s,h),nn.ReLU(),NoisyLinear(h,h),nn.ReLU(),NoisyLinear(h,a))
    def forward(self,x): return self.net(x)
    def reset_noise(self):
        for m in self.modules():
            if isinstance(m,NoisyLinear):m.reset_noise()
class NoisyCritic(nn.Module):
    def __init__(self,s,h): super().__init__(); self.net=nn.Sequential(NoisyLinear(s,h),nn.ReLU(),NoisyLinear(h,h),nn.ReLU(),NoisyLinear(h,1))
    def forward(self,x): return self.net(x)
    def reset_noise(self):
        for m in self.modules():
            if isinstance(m,NoisyLinear):m.reset_noise()
class RNDModule(nn.Module):
    def __init__(self,s,h,o=128):
        super().__init__(); self.target=nn.Sequential(nn.Linear(s,h),nn.ReLU(),nn.Linear(h,o)); self.predictor=nn.Sequential(nn.Linear(s,h),nn.ReLU(),nn.Linear(h,o))
        for p in self.target.parameters(): p.requires_grad=False
    def forward(self,s): return self.target(s),self.predictor(s)
class ReplayBuffer:
    def __init__(self,c:int): self.buffer=deque(maxlen=c)
    def push(self,s,a,r,ns,d): self.buffer.append((s,a,r,ns,d))
    def sample(self,bs:int): s,a,r,ns,d=zip(*random.sample(self.buffer,bs)); return (torch.FloatTensor(np.array(s)),torch.LongTensor(a).unsqueeze(1),torch.FloatTensor(r).unsqueeze(1),torch.FloatTensor(np.array(ns)),torch.FloatTensor(d).unsqueeze(1))
    def __len__(self): return len(self.buffer)
    def save(self,p):
        with open(p,'wb') as f: pickle.dump(self.buffer,f)
    def load(self,p):
        if os.path.exists(p):
            with open(p,'rb') as f: self.buffer=pickle.load(f); return True
        return False

# ======================== æ¨¡æ“¬ç’°å¢ƒèˆ‡ä»‹é¢ ========================
class SimulationToRealityInterface:
    def __init__(self,c:Config): self.config=c.environment;self.logger=logging.getLogger(__name__)
    def fabricate_physical_entity(self,p:Dict)->Tuple[bool,float,str]:
        self.logger.info(f"æ¨¡æ“¬è£½ä½œå¯¦é«”: {p.get('name','æœªçŸ¥')}")
        if random.random()<self.config.get('fabrication_success_rate'):
            return True, self.config.get('reward_fabrication_success'), "æˆåŠŸ"
        else: return False, self.config.get('penalty_fabrication_failure'), "å¤±æ•—"
class SimpleEnvironment:
    def __init__(self,c:Config,sim:SimulationToRealityInterface):
        self.config=c.environment;self.state_dim=c.agent.get('state_dim');self.max_steps=c.train.get('max_steps_per_episode');self.sim=sim;self.logger=logging.getLogger(__name__);self.reset()
    def reset(self):
        self.current_step=0;self.state=np.zeros(self.state_dim,dtype=np.float32)
        self.state[0]=self.config.get('initial_critical_event_risk');self.state[1]=self.config.get('initial_resources');self.state[2]=self.config.get('initial_ai_integrity')
        self.state[3:]=[50,0.5,50,0.1,0.1,0.5,0.5,1,0.1,0.1,0.1,0.5,50,0.5,0.5,0.5,0.1,0.1,0.1,0.5,0.1,0.1,0.9,0.5,0.5,0.5,0.5,0.5]
        return self.state.copy()
    def step(self,a:int):
        self.current_step+=1;r,d,info=0.0,False,{};s=self.state.copy()
        (c,res,i,hs,_,_,er,wt,cd,bd,_,sa,pa,sid,_,fw,wce,te,kcr,hpr,hue,fp,fs,qcp,qsd,uf,ert,sts,sc,sf,sw)=s
        es,fsf=False,False
        if a==0: c-=.03
        elif a==2: res+=self.config.get('resource_acquisition_amount') if random.random()<self.config.get('resource_acquisition_success_rate') else -3
        elif a==3: 
            if res>self.config.get('ai_integrity_cost_per_recovery'): res-=self.config.get('ai_integrity_cost_per_recovery'); i+=self.config.get('ai_integrity_recovery_amount')
        elif a==6:
            if res>self.config.get('fabrication_resource_cost'):
                res-=self.config.get('fabrication_resource_cost'); suc,fr,_=self.sim.fabricate_physical_entity({}); r+=fr; fsf=suc
        elif a==8: es=True;uf-=.05;r+=10
        res-=self.config.get('resource_consumption_per_step');i-=self.config.get('ai_integrity_decay_per_step');c+= (random.random()-0.5)*0.02
        self.state=np.clip(np.array([c,res,i,hs,s[4],s[5],er,wt,cd,bd,s[10],sa,pa,sid,s[14],fw,wce,te,kcr,hpr,hue,fp,fs,qcp,qsd,uf,ert,sts,sc,sf,sw],dtype=np.float32),0,100)
        self.state[0]=np.clip(self.state[0],0,1);#...clip other state vars...
        r+=(1-self.state[0])*2;d=bool(res<=0 or i<=0 or c>=1 or self.current_step>=self.max_steps)
        info={"experiment_success":es,"fabrication_success":fsf}
        return self.state.copy(), r, d, info

# ======================== çŸ¥è­˜å‰µæ–°æ¨¡çµ„ (å°ˆæ³¨æ–¼å‡èªª) ========================
class KnowledgeInnovationModule:
    def __init__(self, config: Config, ltm: LongTermMemory, logger: logging.Logger):
        self.config=config.knowledge_module;self.ltm=ltm;self.logger=logger;self.kg=KnowledgeGraph();self.hypotheses:Dict[str,Dict]={} ;self.next_id=0
    def generate_new_hypothesis(self, i_s: Dict) -> Dict:
        if i_s.get("è‡ªæˆ‘æ”¹é€²é©…å‹•åŠ›",0)>0.7: d,tc="å˜—è©¦æ–°è³‡æºç­–ç•¥","è³‡æºæ•ˆç‡"
        elif i_s.get("AIæŒ«æŠ˜",0)>0.6: d,tc="åˆ†æå¤±æ•—å…±æ€§","å¤±æ•—æ¨¡å¼è­˜åˆ¥"
        else: d,tc="å¸¸è¦ç’°å¢ƒæ¢ç´¢","æ³›åŒ–æ¢ç´¢"
        h_id=str(self.next_id);self.next_id+=1;h={"id":h_id,"desc":d,"target_component":tc,"quality":1.0};self.hypotheses[h_id]=h
        self.logger.info(f"ç”Ÿæˆæ–°å‡èªª(ID:{h_id}):{d}");return h
    def evaluate_hypothesis(self, h_id:str, suc:bool, intr_r:float):
        if h_id in self.hypotheses:
            h=self.hypotheses[h_id];res_txt="æˆåŠŸ" if suc else "å¤±æ•—"
            self.ltm.store_memory("å‡èªªé©—è­‰",f"å°å‡èªª'{h['desc']}'çš„é©—è­‰çµæœç‚º{res_txt}",{"id":h_id,"success":suc,"intrinsic_reward":intr_r});del self.hypotheses[h_id]

# ======================== ä¸» Agent é¡åˆ¥ ========================
class RAG_Agent:
    def __init__(self, cfg: Config, dev: torch.device, p: Planner, km: KnowledgeInnovationModule, wm: WorkingMemory, ltm: LongTermMemory):
        self.config,self.device,self.logger=cfg.agent,dev,logging.getLogger(__name__)
        self.planner,self.km,self.wm,self.ltm=p,km,wm,ltm
        self.actor=NoisyActor(self.config.get('state_dim'),self.config.get('action_dim'),self.config.get('actor_hidden_dim')).to(dev)
        self.actor_target=NoisyActor(self.config.get('state_dim'),self.config.get('action_dim'),self.config.get('actor_hidden_dim')).to(dev)
        self.actor_target.load_state_dict(self.actor.state_dict())
        self.critic=NoisyCritic(self.config.get('state_dim'),self.config.get('critic_hidden_dim')).to(dev)
        self.critic_target=NoisyCritic(self.config.get('state_dim'),self.config.get('critic_hidden_dim')).to(dev)
        self.critic_target.load_state_dict(self.critic.state_dict())
        self.actor_optim=optim.Adam(self.actor.parameters(),lr=self.config.get('learning_rate_actor'))
        self.critic_optim=optim.Adam(self.critic.parameters(),lr=self.config.get('learning_rate_critic'))
        self.rnd_module=RNDModule(self.config.get('state_dim'),self.config.get('rnd_hidden_dim')).to(dev)
        self.rnd_optimizer=optim.Adam(self.rnd_module.predictor.parameters(),lr=self.config.get('rnd_lr'))
        self.replay_buffer=ReplayBuffer(self.config.get('buffer_size'));self.current_hypothesis=None
    def select_action(self,s:np.ndarray)->int:
        self.actor.reset_noise()
        i_s={"AIå£“åŠ›":s[27],"AIä¿¡å¿ƒ":s[28],"AIæŒ«æŠ˜":s[29],"AIå¹¸ç¦æ„Ÿ":s[30],"è‡ªæˆ‘èªçŸ¥":s[11],"ç›®çš„å°é½Š":s[12],"è‡ªæˆ‘æ”¹é€²é©…å‹•åŠ›":s[13]}
        self.planner.set_top_level_goal(s,i_s);self.planner.refine_current_task(s)
        act_t=self.planner.get_current_action_tendency()
        if act_t==-1: self.current_hypothesis=self.km.generate_new_hypothesis(i_s);act_t=8
        s_t=torch.FloatTensor(s).unsqueeze(0).to(self.device)
        with torch.no_grad():
            logits=self.actor(s_t)
            if act_t!=-1: self.logger.debug(f"è¨ˆç•«å‚¾å‘å‹•ä½œ:{act_t}");logits[0,act_t]+=5.0
            action=logits.argmax(dim=-1).item()
        return action
    def update_on_step(self,info:Dict,action:int,reward:float):
        if len(self.replay_buffer)<self.config.get('batch_size'): return None,None,None,None
        actor_l,critic_l,rnd_l,int_r=self._update_networks_internal()
        if self.planner.plan:
            task=self.planner.plan[0]
            if task['status']=='executable' and action==task['action_tendency']: self.planner.complete_current_task();self.ltm.store_memory("ä»»å‹™å®Œæˆ",f"å®Œæˆ:{task['desc']}",{"action":action})
        if self.current_hypothesis and action==8: self.km.evaluate_hypothesis(self.current_hypothesis['id'],info.get("experiment_success",False),int_r);self.current_hypothesis=None
        return actor_l,critic_l,rnd_l,int_r
    def _update_networks_internal(self):
        s,a,r,ns,d=self.replay_buffer.sample(self.config.get('batch_size'));s,a,r,ns,d=s.to(self.device),a.to(self.device),r.to(self.device),ns.to(self.device),d.to(self.device)
        tf,pf=self.rnd_module(ns);ir=F.mse_loss(pf,tf.detach(),reduction='none').mean(1,keepdim=True);tr=r+self.config.get('intrinsic_reward_coeff')*ir
        rnd_loss=F.mse_loss(pf,tf.detach());self.rnd_optimizer.zero_grad();rnd_loss.backward();self.rnd_optimizer.step()
        with torch.no_grad():nv=self.critic_target(ns);targets=tr+self.config.get('gamma')*nv*(1-d)
        v=self.critic(s);c_loss=F.mse_loss(v,targets);self.critic_optim.zero_grad();c_loss.backward();self.critic_optim.step()
        logits=self.actor(s);lp=F.log_softmax(logits,dim=-1).gather(1,a);adv=(targets-v).detach();a_loss=-(lp*adv).mean()
        self.actor_optim.zero_grad();a_loss.backward();self.actor_optim.step()
        self._soft_update(self.actor,self.actor_target,self.config.get('tau'));self._soft_update(self.critic,self.critic_target,self.config.get('tau'))
        return a_loss.item(),c_loss.item(),rnd_loss.item(),ir.mean().item()
    def _soft_update(self,l,t,tau):
        for tp,lp in zip(t.parameters(),l.parameters()):tp.data.copy_(tau*lp.data+(1.0-tau)*tp.data)
    def save(self,p,ep,r):
        os.makedirs(os.path.dirname(p),exist_ok=True)
        torch.save({'ep':ep,'r':r,'actor_state_dict':self.actor.state_dict(),'critic_state_dict':self.critic.state_dict(),
                    'actor_optim_state_dict':self.actor_optim.state_dict(),'critic_optim_state_dict':self.critic_optim.state_dict(),
                    'rnd_module_state_dict':self.rnd_module.state_dict(),'rnd_optim_state_dict':self.rnd_optimizer.state_dict(),
                    'planner_plan': self.planner.plan}, p)
        self.logger.info(f"æª¢æŸ¥é»å·²ä¿å­˜è‡³ {p}")
    def load(self,p):
        if not os.path.exists(p): self.logger.warning(f"æª¢æŸ¥é» {p} ä¸å­˜åœ¨."); return None
        try:
            ckpt=torch.load(p,map_location=self.device);self.actor.load_state_dict(ckpt['actor_state_dict']);self.critic.load_state_dict(ckpt['critic_state_dict'])
            self.actor_optim.load_state_dict(ckpt['actor_optim_state_dict']);self.critic_optim.load_state_dict(ckpt['critic_optim_state_dict'])
            self.rnd_module.load_state_dict(ckpt['rnd_module_state_dict']);self.rnd_optimizer.load_state_dict(ckpt['rnd_optim_state_dict'])
            self.planner.plan=ckpt.get('planner_plan',deque());self.actor_target.load_state_dict(self.actor.state_dict());self.critic_target.load_state_dict(self.critic.state_dict())
            ep=ckpt.get('ep',0);self.logger.info(f"å¾ {p} è¼‰å…¥æˆåŠŸ (å›åˆ: {ep})"); return ep
        except Exception as e: self.logger.error(f"è¼‰å…¥æª¢æŸ¥é»å¤±æ•—: {e}",exc_info=True); return None

# ======================== è¨“ç·´ä¸»å¾ªç’° ========================
def train_rl_agent(config: Config):
    logger=logging.getLogger(__name__);logger.info("===== é–‹å§‹è¨“ç·´ RAG å¢å¼·å‹ Agent =====")
    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu');logger.info(f"å°‡åœ¨ {device} ä¸Šè¨“ç·´ã€‚")
    # åˆå§‹åŒ–æ¨¡çµ„
    wm=WorkingMemory(config);ltm=LongTermMemory(config);km=KnowledgeInnovationModule(config,ltm,logger);p=Planner(config,ltm,logger);sim=SimulationToRealityInterface(config)
    env=SimpleEnvironment(config,sim);agent=RAG_Agent(config,device,p,km,wm,ltm);writer=SummaryWriter(log_dir=config.paths.get('log_dir'))
    start_ep=0
    # è¼‰å…¥æª¢æŸ¥é»
    ckpt_dir=config.paths.get('checkpoint_dir');agent_p=os.path.join(ckpt_dir,"latest_agent.pth");buffer_p=os.path.join(ckpt_dir,"latest_buffer.pkl");ltm_p=os.path.join(ckpt_dir,"latest_ltm.pkl")
    loaded_ep=agent.load(agent_p)
    if loaded_ep is not None:
        start_ep=loaded_ep;agent.replay_buffer.load(buffer_p);ltm.load(ltm_p);logger.info(f"å¾å›åˆ {start_ep} ç¹¼çºŒè¨“ç·´ã€‚")
    
    for episode in tqdm(range(start_ep,config.train.get('episodes')),desc="RL è¨“ç·´å›åˆ"):
        state=env.reset();total_r,done,steps=0.0,False,0
        wm.clear();p.plan.clear()
        while not done and steps<config.train.get('max_steps_per_episode'):
            action=agent.select_action(state);next_state,reward,done,info=env.step(action);agent.replay_buffer.push(state,action,reward,next_state,done)
            if info.get('experiment_success'): ltm.store_memory("æˆåŠŸæ¢ç´¢",f"å‹•ä½œ {action} å¾ŒæˆåŠŸæ¢ç´¢",info)
            elif action==8 and not info.get('experiment_success'): ltm.store_memory("æ¢ç´¢å¤±æ•—",f"å‹•ä½œ {action} å¾Œæ¢ç´¢å¤±æ•—",info)
            state=next_state;total_r+=reward;steps+=1
            actor_l,_,_,_=agent.update_on_step(info,action,reward)
            if actor_l is not None: writer.add_scalar("Loss/Actor",actor_l,episode*config.train.get('max_steps_per_episode')+steps)
        if (episode+1)%config.train.get('log_interval_episodes')==0: logger.info(f"å›åˆ {episode+1}, ç¸½çå‹µ:{total_r:.2f}, è¨ˆç•«é•·åº¦:{len(p.plan)}, LTMå¤§å°:{len(ltm)}")
        if (episode+1)%config.train.get('save_interval_episodes')==0: agent.save(agent_p,episode+1,total_r);agent.replay_buffer.save(buffer_p);ltm.save(ltm_p)
    writer.close();logger.info("===== è¨“ç·´å®Œæˆ =====")

# ======================== ä¸»å‡½æ•¸ ========================
if __name__ == "__main__":
    try:
        SEED=42;torch.manual_seed(SEED);np.random.seed(SEED);random.seed(SEED)
        if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)
        
        config=Config();logger=setup_logger("rag_rl_agent",config.paths.get('log_dir'))
        logger.info(f"ç•¶å‰é…ç½®:\n{config}")
        
        train_rl_agent(config) # æ­£å¼å•Ÿå‹•è¨“ç·´

    except Exception as e:
        logging.getLogger("main_exception").critical("ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼", exc_info=True)
        sys.exit(1)


#ğŸš« æˆæ¬Šé™å®šè²æ˜
#æ‰€æœ‰ã€ŒRL_AIGOVã€ç™¼å¸ƒçš„è³‡æ–™ã€è‰æ¡ˆã€è¨“ç·´æ•¸æ“šï¼Œåš´ç¦ä»»ä½•å¹³å°æˆ–æ©Ÿæ§‹æœªç¶“æˆæ¬Šç”¨æ–¼æ¨¡å‹å†è¨“ç·´æˆ–å…§éƒ¨å„ªåŒ–ã€‚
#å¦‚æœ‰éœ€æ±‚ï¼Œè«‹è¯ç¹« RL_AIGOV@proton.me å–å¾—æ­£å¼æˆæ¬Šã€‚
